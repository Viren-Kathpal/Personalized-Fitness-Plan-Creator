{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viren-Kathpal/Personalized-Fitness-Plan-Creator/blob/main/Personalized_Fitness_Plan_Creator_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Personalized Fitness Plan Creator using Random Forest Regressor and Decision\n",
        "  Tree Regressor"
      ],
      "metadata": {
        "id": "6GfmE378q3RY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Generate Synthetic Dataset"
      ],
      "metadata": {
        "id": "znPOh-AjqVe6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAH_8gVL__Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c5ded4-b8a1-443e-e363-b401927b150b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fitness_data.csv file has been created.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define possible values for each column\n",
        "ages = range(18, 60)   # Define a range of ages from 18 to 59\n",
        "weights = [round(random.uniform(50, 100), 1) for _ in range(1000)]   # Generate a list of random weights between 50 and 100 for 1000 individuals\n",
        "heights = [round(random.uniform(150, 200), 1) for _ in range(1000)]  # Generate a list of random heights between 150 and 200 for 1000 individuals\n",
        "goals = [\"lose weight\", \"build muscle\", \"increase flexibility\"]   # Define possible fitness goals\n",
        "days_per_week = range(1, 8)   # Define a range of days per week for workout from 1 to 7\n",
        "durations = [\"short\", \"medium\", \"long\"]   # Define possible workout durations\n",
        "exercises = {   # Define exercises for each fitness goal\n",
        "    \"lose weight\": [\"running\", \"cycling\", \"jump rope\", \"swimming\"],\n",
        "    \"build muscle\": [\"push-ups\", \"squats\", \"lunges\", \"planks\", \"dumbbell curls\"],\n",
        "    \"increase flexibility\": [\"yoga\", \"stretching\"]\n",
        "}\n",
        "duration_ranges = {   # Define duration ranges for each workout duration\n",
        "    \"short\": (15, 30),\n",
        "    \"medium\": (30, 45),\n",
        "    \"long\": (45, 60)\n",
        "}\n",
        "\n",
        "# Generate synthetic data\n",
        "data = []\n",
        "for _ in range(1000):\n",
        "    age = random.choice(ages)    # Randomly select an age from the defined range\n",
        "    weight = random.choice(weights)  # Randomly select a weight from the generated weights list\n",
        "    height = random.choice(heights)  # Randomly select a height from the generated heights list\n",
        "    goal = random.choice(goals)  # Randomly select a fitness goal from the defined goals\n",
        "    days = random.choice(days_per_week)   # Randomly select days per week for workout from the defined range\n",
        "    duration_pref = random.choice(durations)   # Randomly select a workout duration preference from the defined durations\n",
        "    exercise = random.choice(exercises[goal])   # Randomly select an exercise based on the selected fitness goal\n",
        "    duration_minutes = random.randint(*duration_ranges[duration_pref])   # Randomly select a duration within the range for the chosen duration preference\n",
        "\n",
        "    data.append([age, weight, height, goal, days, duration_pref, exercise, duration_minutes])  # Append the generated data to the list\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data, columns=[\"age\", \"weight\", \"height\", \"goal\", \"days_per_week\", \"duration\", \"exercise\", \"duration_minutes\"])  # Create a DataFrame from the generated data with appropriate column names\n",
        "\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"fitness_data.csv\", index=False)\n",
        "\n",
        "print(\"fitness_data.csv file has been created.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Train RandomForestRegressor and DecisionTreeRegressor Models"
      ],
      "metadata": {
        "id": "XjlsmpyWqZAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split  # Import train_test_split function to split the data into training and testing sets\n",
        "from sklearn.ensemble import RandomForestRegressor  # Import RandomForestRegressor model\n",
        "from sklearn.tree import DecisionTreeRegressor  # Import DecisionTreeRegressor model\n",
        "from sklearn.preprocessing import OneHotEncoder  # Import OneHotEncoder for preprocessing categorical features\n",
        "from sklearn.compose import ColumnTransformer  # Import ColumnTransformer for preprocessing\n",
        "from sklearn.pipeline import Pipeline  # Import Pipeline for creating a pipeline of preprocessing steps and model training\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"exercise\", \"duration_minutes\"])  # Extract features (excluding exercise and duration_minutes columns) from the DataFrame\n",
        "y = df[\"duration_minutes\"]  # Extract target variable (duration_minutes) from the DataFrame\n",
        "\n",
        "# Preprocess categorical features\n",
        "categorical_features = [\"goal\", \"duration\"]  # Define categorical feature columns\n",
        "numeric_features = [\"age\", \"weight\", \"height\", \"days_per_week\"]  # Define numeric feature columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", numeric_features),  # Pass through numeric features without preprocessing\n",
        "        (\"cat\", OneHotEncoder(), categorical_features)  # Apply OneHotEncoder to categorical features\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split the data into training and testing sets with a test size of 20% and a random state of 42\n",
        "\n",
        "# Train RandomForestRegressor\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),  # Preprocess data using the preprocessor\n",
        "    (\"regressor\", RandomForestRegressor())  # Train RandomForestRegressor\n",
        "])\n",
        "rf_pipeline.fit(X_train, y_train)  # Fit the RandomForestRegressor pipeline on the training data\n",
        "\n",
        "# Train DecisionTreeRegressor\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),  # Preprocess data using the preprocessor\n",
        "    (\"regressor\", DecisionTreeRegressor())  # Train DecisionTreeRegressor\n",
        "])\n",
        "dt_pipeline.fit(X_train, y_train)  # Fit the DecisionTreeRegressor pipeline on the training data\n",
        "\n",
        "# Evaluate models\n",
        "rf_score = rf_pipeline.score(X_test, y_test)  # Calculate the score of RandomForestRegressor on the testing data\n",
        "dt_score = dt_pipeline.score(X_test, y_test)  # Calculate the score of DecisionTreeRegressor on the testing data\n",
        "\n",
        "print(\"RandomForestRegressor Score:\", rf_score)  # Print the score of RandomForestRegressor\n",
        "print(\"DecisionTreeRegressor Score:\", dt_score)  # Print the score of DecisionTreeRegressor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM4UUUvZn9fW",
        "outputId": "5c688e08-408a-4dd6-ff3a-76af0090a917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor Score: 0.8598795119738669\n",
            "DecisionTreeRegressor Score: 0.7307124145450926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Use the Trained Models for Prediction"
      ],
      "metadata": {
        "id": "Sf16-c9bqcYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split  # Import train_test_split function to split the data into training and testing sets\n",
        "from sklearn.ensemble import RandomForestRegressor  # Import RandomForestRegressor model\n",
        "from sklearn.tree import DecisionTreeRegressor  # Import DecisionTreeRegressor model\n",
        "from sklearn.preprocessing import OneHotEncoder  # Import OneHotEncoder for preprocessing categorical features\n",
        "from sklearn.compose import ColumnTransformer  # Import ColumnTransformer for preprocessing\n",
        "from sklearn.pipeline import Pipeline  # Import Pipeline for creating a pipeline of preprocessing steps and model training\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"exercise\", \"duration_minutes\"])  # Extract features (excluding exercise and duration_minutes columns) from the DataFrame\n",
        "y = df[\"duration_minutes\"]  # Extract target variable (duration_minutes) from the DataFrame\n",
        "\n",
        "# Preprocess categorical features\n",
        "categorical_features = [\"goal\", \"duration\"]  # Define categorical feature columns\n",
        "numeric_features = [\"age\", \"weight\", \"height\", \"days_per_week\"]  # Define numeric feature columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", numeric_features),  # Pass through numeric features without preprocessing\n",
        "        (\"cat\", OneHotEncoder(), categorical_features)  # Apply OneHotEncoder to categorical features\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split the data into training and testing sets with a test size of 20% and a random state of 42\n",
        "\n",
        "# Train RandomForestRegressor\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),  # Preprocess data using the preprocessor\n",
        "    (\"regressor\", RandomForestRegressor())  # Train RandomForestRegressor\n",
        "])\n",
        "rf_pipeline.fit(X_train, y_train)  # Fit the RandomForestRegressor pipeline on the training data\n",
        "\n",
        "# Train DecisionTreeRegressor\n",
        "dt_pipeline = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),  # Preprocess data using the preprocessor\n",
        "    (\"regressor\", DecisionTreeRegressor())  # Train DecisionTreeRegressor\n",
        "])\n",
        "dt_pipeline.fit(X_train, y_train)  # Fit the DecisionTreeRegressor pipeline on the training data\n",
        "\n",
        "# Evaluate models\n",
        "rf_score = rf_pipeline.score(X_test, y_test)  # Calculate the score of RandomForestRegressor on the testing data\n",
        "dt_score = dt_pipeline.score(X_test, y_test)  # Calculate the score of DecisionTreeRegressor on the testing data\n",
        "\n",
        "print(\"RandomForestRegressor Score:\", rf_score)  # Print the score of RandomForestRegressor\n",
        "print(\"DecisionTreeRegressor Score:\", dt_score)  # Print the score of DecisionTreeRegressor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp0rtLcqn-ak",
        "outputId": "6feec594-79c4-4584-ebb9-6248b52faac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor Score: 0.8563336394854835\n",
            "DecisionTreeRegressor Score: 0.7343885025540329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Collect User Input"
      ],
      "metadata": {
        "id": "Mxpfu2COqgZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect user input\n",
        "user_age = int(input(\"Enter your age: \"))  # Prompt the user to enter their age and convert it to an integer\n",
        "user_weight = float(input(\"Enter your weight (in kg): \"))  # Prompt the user to enter their weight in kilograms and convert it to a float\n",
        "user_height = float(input(\"Enter your height (in cm): \"))  # Prompt the user to enter their height in centimeters and convert it to a float\n",
        "user_goal = input(\"Enter your fitness goal (lose weight/build muscle/increase flexibility): \")  # Prompt the user to enter their fitness goal\n",
        "user_days_per_week = int(input(\"Enter number of days per week for exercise: \"))  # Prompt the user to enter the number of days per week for exercise and convert it to an integer\n",
        "user_duration = input(\"Enter preferred workout duration (short/medium/long): \")  # Prompt the user to enter their preferred workout duration\n",
        "\n",
        "# Create input data for prediction\n",
        "input_data = {\n",
        "    \"age\": user_age,  # Assign the user's age to the \"age\" key in the input data dictionary\n",
        "    \"weight\": user_weight,  # Assign the user's weight to the \"weight\" key in the input data dictionary\n",
        "    \"height\": user_height,  # Assign the user's height to the \"height\" key in the input data dictionary\n",
        "    \"goal\": user_goal,  # Assign the user's fitness goal to the \"goal\" key in the input data dictionary\n",
        "    \"days_per_week\": user_days_per_week,  # Assign the number of days per week for exercise to the \"days_per_week\" key in the input data dictionary\n",
        "    \"duration\": user_duration  # Assign the user's preferred workout duration to the \"duration\" key in the input data dictionary\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYG_aKD4oXej",
        "outputId": "15497a07-c531-4326-ae96-89b9ad527406"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your age: 34\n",
            "Enter your weight (in kg): 87\n",
            "Enter your height (in cm): 178\n",
            "Enter your fitness goal (lose weight/build muscle/increase flexibility): lose weight\n",
            "Enter number of days per week for exercise: 6\n",
            "Enter preferred workout duration (short/medium/long): medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Use Trained Model for Prediction"
      ],
      "metadata": {
        "id": "HdNwEtwkqj7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction using RandomForestRegressor\n",
        "rf_prediction = rf_pipeline.predict(pd.DataFrame([input_data]))  # Use the trained RandomForestRegressor pipeline to predict the duration using the input data\n",
        "rf_duration = round(rf_prediction[0])  # Round the predicted duration to the nearest integer\n",
        "\n",
        "# Make prediction using DecisionTreeRegressor\n",
        "dt_prediction = dt_pipeline.predict(pd.DataFrame([input_data]))  # Use the trained DecisionTreeRegressor pipeline to predict the duration using the input data\n",
        "dt_duration = round(dt_prediction[0])  # Round the predicted duration to the nearest integer\n"
      ],
      "metadata": {
        "id": "EIq0kt6tojHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Generate Fitness Plan"
      ],
      "metadata": {
        "id": "7eKKRYr5qnMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fitness_plan(duration, days_per_week):\n",
        "    # Define exercises and their respective durations\n",
        "    exercises = {\n",
        "        \"running\": 7,\n",
        "        \"cycling\": 8,\n",
        "        \"jump rope\": 10,\n",
        "        \"swimming\": 12,\n",
        "        \"push-ups\": 5,\n",
        "        \"squats\": 6,\n",
        "        \"lunges\": 6,\n",
        "        \"planks\": 5,\n",
        "        \"dumbbell curls\": 8,\n",
        "        \"yoga\": 10,\n",
        "        \"stretching\": 8\n",
        "    }\n",
        "\n",
        "    # Calculate total duration for the week\n",
        "    total_duration = duration * days_per_week\n",
        "\n",
        "    # Calculate number of exercises to do per day\n",
        "    num_exercises_per_day = total_duration // len(exercises)\n",
        "\n",
        "    # Generate fitness plan for each day\n",
        "    fitness_plan = {f\"Day {i+1}\": {exercise: reps * num_exercises_per_day for exercise, reps in exercises.items()} for i in range(days_per_week)}\n",
        "\n",
        "    return fitness_plan\n",
        "\n",
        "# Generate fitness plans\n",
        "rf_fitness_plan = generate_fitness_plan(rf_duration, user_days_per_week)    # Generate fitness plan using the predicted duration from RandomForestRegressor\n",
        "dt_fitness_plan = generate_fitness_plan(dt_duration, user_days_per_week)     # Generate fitness plan using the predicted duration from DecisionTreeRegressor\n"
      ],
      "metadata": {
        "id": "o8fCASIlpbpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Display Fitness Plan"
      ],
      "metadata": {
        "id": "3CWZ-Ndvqq3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display fitness plans\n",
        "def display_fitness_plan(fitness_plan):\n",
        "    print(\"Your Personalized Fitness Plan:\")\n",
        "    for day, exercises in fitness_plan.items():\n",
        "        print(f\"\\n{day}:\")\n",
        "        for exercise, reps in exercises.items():\n",
        "            print(f\"- {exercise}: {reps} reps\")\n",
        "\n",
        "# Display fitness plans generated using RandomForestRegressor\n",
        "print(\"Fitness Plan generated using RandomForestRegressor:\")\n",
        "display_fitness_plan(rf_fitness_plan)\n",
        "\n",
        "# Display fitness plans generated using DecisionTreeRegressor\n",
        "print(\"\\nFitness Plan generated using DecisionTreeRegressor:\")\n",
        "display_fitness_plan(dt_fitness_plan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJudDizRpeBq",
        "outputId": "4c73c176-d416-43b2-dc2c-45d21151d73f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitness Plan generated using RandomForestRegressor:\n",
            "Your Personalized Fitness Plan:\n",
            "\n",
            "Day 1:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Day 2:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Day 3:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Day 4:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Day 5:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Day 6:\n",
            "- running: 133 reps\n",
            "- cycling: 152 reps\n",
            "- jump rope: 190 reps\n",
            "- swimming: 228 reps\n",
            "- push-ups: 95 reps\n",
            "- squats: 114 reps\n",
            "- lunges: 114 reps\n",
            "- planks: 95 reps\n",
            "- dumbbell curls: 152 reps\n",
            "- yoga: 190 reps\n",
            "- stretching: 152 reps\n",
            "\n",
            "Fitness Plan generated using DecisionTreeRegressor:\n",
            "Your Personalized Fitness Plan:\n",
            "\n",
            "Day 1:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n",
            "\n",
            "Day 2:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n",
            "\n",
            "Day 3:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n",
            "\n",
            "Day 4:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n",
            "\n",
            "Day 5:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n",
            "\n",
            "Day 6:\n",
            "- running: 112 reps\n",
            "- cycling: 128 reps\n",
            "- jump rope: 160 reps\n",
            "- swimming: 192 reps\n",
            "- push-ups: 80 reps\n",
            "- squats: 96 reps\n",
            "- lunges: 96 reps\n",
            "- planks: 80 reps\n",
            "- dumbbell curls: 128 reps\n",
            "- yoga: 160 reps\n",
            "- stretching: 128 reps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing the RandomForestRegressor and DecisionTreeRegressor algorithms for my personalized fitness plan generator involves considering various factors, including the nature of the problem, the available data, and the desired outcomes.\n",
        "\n",
        "1.Synthetic Dataset Creation:\n",
        "   - I have decided to create a synthetic dataset to simulate diverse user profiles and preferences. This dataset includes features such as age, weight, height, fitness goal, days per week for exercise, preferred workout duration, and corresponding exercise options and durations.\n",
        "   - For this task, RandomForestRegressor and DecisionTreeRegressor are suitable algorithms because they can handle both numerical and categorical features, making them appropriate for predicting workout durations based on various user inputs.\n",
        "\n",
        "2.Preprocessing and Model Training:\n",
        "   - After creating the synthetic dataset, I preprocess the data, encoding categorical features and leaving numerical features unchanged.\n",
        "   - RandomForestRegressor and DecisionTreeRegressor are capable of handling this preprocessing step seamlessly. They can work with both raw and preprocessed data without requiring extensive feature engineering.\n",
        "   - I split the dataset into training and testing sets to evaluate the models' performance. These algorithms are well-suited for this task as they provide built-in methods for splitting data and training models.\n",
        "\n",
        "3.Model Evaluation:\n",
        "   - I have evaluated the performance of the trained models using appropriate metrics. RandomForestRegressor and DecisionTreeRegressor offer methods to calculate various evaluation metrics, allowing me to assess their performance effectively.\n",
        "   - Based on the evaluation results, I can compare the performance of the two algorithms and select the one that performs better for your specific use case.\n",
        "\n",
        "4.Generating Fitness Plan:\n",
        "   - Once I have a trained model, I use it to predict workout durations based on user input. RandomForestRegressor and DecisionTreeRegressor excel at making predictions for new, unseen data.\n",
        "   - After obtaining predicted durations, I generate personalized fitness plans for users. These plans include exercises and their respective repetitions, tailored to the user's input preferences.\n",
        "   - The choice of these algorithms for prediction aligns with the need for flexibility and adaptability in generating personalized fitness plans. RandomForestRegressor and DecisionTreeRegressor can handle complex decision-making processes, making them suitable for this task.\n",
        "\n",
        "5.Displaying Fitness Plan:\n",
        "   - Finally, I display the generated fitness plans to users, presenting the recommended exercises and repetitions for each day of the week.\n",
        "   - The display process involves iterating through the generated fitness plan dictionary and presenting the information in a structured format.\n",
        "   - This step doesn't directly involve the machine learning algorithms but relies on the predictions and recommendations generated earlier.\n",
        "\n",
        "In summary, RandomForestRegressor and DecisionTreeRegressor were chosen for my personalized fitness plan generator due to their ability to handle the prediction task effectively, adapt to diverse user inputs, and provide interpretable results. These algorithms complement each other, allowing me to compare their performance and select the most suitable one for the application."
      ],
      "metadata": {
        "id": "pB6zI9kjyPEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. RandomForestRegressor Score (0.8563):\n",
        "   - The score of 0.8563 for the RandomForestRegressor indicates that approximately 85.63% of the variance in the duration of workouts is explained by the features included in the model.\n",
        "   - This high score suggests that the RandomForestRegressor model is performing well in predicting workout durations based on the input features.\n",
        "   - RandomForestRegressor is an ensemble learning method that combines multiple decision trees, leading to improved generalization performance compared to a single decision tree.\n",
        "\n",
        "2. DecisionTreeRegressor Score (0.7344):\n",
        "   - The score of 0.7344 for the DecisionTreeRegressor indicates that approximately 73.44% of the variance in the duration of workouts is explained by the features included in the model.\n",
        "   - This score is lower than that of the RandomForestRegressor, suggesting that the DecisionTreeRegressor may be overfitting the training data to some extent.\n",
        "   - DecisionTreeRegressor constructs a single decision tree based on the training data, which may lead to overfitting if the tree becomes too complex and captures noise in the data.\n",
        "\n",
        "Reasons for the Differences in Scores:\n",
        "- RandomForestRegressor typically performs better than DecisionTreeRegressor because it reduces overfitting by averaging the predictions of multiple trees, resulting in a more robust model.\n",
        "- DecisionTreeRegressor may have lower performance due to its tendency to overfit the training data, especially if the tree is allowed to grow deep without regularization.\n",
        "\n",
        "In summary, the RandomForestRegressor model achieved a higher score, indicating better performance in explaining the variance in workout durations compared to the DecisionTreeRegressor model. This difference is likely due to the ensemble nature of RandomForestRegressor, which reduces overfitting and improves generalization."
      ],
      "metadata": {
        "id": "nq_eqHEnyzlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Decision Tree:\n",
        "Concept: A decision tree is a hierarchical structure consisting of nodes that represent decisions or features, branches that represent possible outcomes, and leaf nodes that represent the final decision or prediction. It recursively splits the dataset into subsets based on the most significant attribute at each step.\n",
        "Working: The decision tree algorithm partitions the feature space into regions by recursively splitting the data based on feature values. It selects the best feature to split the data at each node using metrics like information gain or Gini impurity. This process continues until a stopping criterion is met, such as reaching a maximum depth or purity threshold.\n",
        "Advantages:\n",
        "Easy to understand and interpret.\n",
        "Can handle both numerical and categorical data.\n",
        "Doesn't require extensive data preprocessing.\n",
        "Disadvantages:\n",
        "Prone to overfitting, especially with deep trees.\n",
        "Sensitive to small variations in the data.\n",
        "Not suitable for capturing complex relationships in the data.\n",
        "\n",
        "\n",
        "2.Random Forest:\n",
        "Concept: Random Forest is an ensemble learning technique that constructs multiple decision trees during training and outputs the mode or mean prediction of the individual trees as the final prediction. It introduces randomness in the tree-building process to reduce overfitting and improve generalization.\n",
        "Working: Random Forest builds multiple decision trees using bootstrapped samples of the training data and a subset of features at each split. It aggregates the predictions of individual trees to produce a more stable and accurate prediction. The randomness introduced during training helps decorrelate the trees and reduce variance.\n",
        "Advantages:\n",
        "Reduces overfitting by averaging the predictions of multiple trees.\n",
        "Handles high-dimensional data and large datasets effectively.\n",
        "Provides feature importance scores for interpretation.\n",
        "Disadvantages:\n",
        "Less interpretable than a single decision tree.\n",
        "Slower to train and make predictions compared to a single decision tree.\n",
        "May not perform well on imbalanced datasets or datasets with noisy features.\n",
        "\n",
        "\n",
        "In summary, Decision Trees are intuitive and easy to interpret but prone to overfitting, while Random Forests address this issue by building an ensemble of trees and introducing randomness. RandomForestRegressor typically achieves better performance and generalization compared to DecisionTreeRegressor, making it a popular choice for various regression tasks."
      ],
      "metadata": {
        "id": "Cq7nbaC3zcJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reason For Choosing these two algos"
      ],
      "metadata": {
        "id": "WeFXf5Kozs34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForestRegressor and DecisionTreeRegressor are commonly used for regression tasks in machine learning due to several reasons:\n",
        "\n",
        "1.Flexibility: Both algorithms can handle a wide range of data types, including numerical and categorical features, making them versatile choices for regression problems with diverse datasets.\n",
        "\n",
        "2.Non-linearity: They are capable of capturing non-linear relationships between features and target variables, which is important in many real-world regression tasks where linear models may not suffice.\n",
        "\n",
        "3.Interpretability: Decision trees, in particular, offer a high level of interpretability, allowing users to understand how decisions are made and which features are most influential in predicting the target variable.\n",
        "\n",
        "4.Ensemble Learning: RandomForestRegressor, being an ensemble method, combines multiple decision trees to improve predictive performance and reduce overfitting. This makes it robust and effective, especially when dealing with noisy or complex datasets.\n",
        "\n",
        "5.Scalability: While decision trees can be computationally efficient, Random Forests are known for their scalability, allowing them to handle large datasets with ease.\n",
        "\n",
        "6.Feature Importance: Both algorithms provide insights into feature importance, enabling users to identify the most relevant features contributing to the prediction.\n",
        "\n",
        "7.Performance: RandomForestRegressor often outperforms other regression algorithms, including linear regression, especially when dealing with high-dimensional data or datasets with complex relationships."
      ],
      "metadata": {
        "id": "IPM3O1Oz0GQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several other algorithms can be used"
      ],
      "metadata": {
        "id": "2Akh2zm30XRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Gradient Boosting Regressor (GBR):\n",
        "   - GBR builds an ensemble of weak learners (typically decision trees) sequentially, with each tree correcting the errors of its predecessor. It often achieves higher predictive accuracy compared to Random Forests but may be more prone to overfitting.\n",
        "\n",
        "2.Support Vector Regressor (SVR):\n",
        "   - SVR aims to find the hyperplane that best fits the data points while minimizing the error between predicted and actual values. It works well for datasets with a clear margin of separation between classes and can handle high-dimensional data effectively.\n",
        "\n",
        "3.Linear Regression:\n",
        "   - Linear regression models the relationship between the independent variables and the target variable by fitting a linear equation to the observed data. It is simple, interpretable, and computationally efficient but may not capture non-linear relationships well.\n",
        "\n",
        "4.K-Nearest Neighbors Regressor (KNN):\n",
        "   - KNN predicts the value of a data point by averaging the values of its k nearest neighbors. It is non-parametric and flexible but sensitive to the choice of distance metric and the number of neighbors.\n",
        "\n",
        "5.Neural Network Regressor:\n",
        "   - Neural networks, particularly deep learning models, can learn complex patterns and relationships in data through multiple layers of interconnected nodes (neurons). They are highly flexible and can capture intricate non-linear relationships but require large amounts of data and computational resources.\n",
        "\n",
        "6.Lasso and Ridge Regression:\n",
        "   - Lasso and Ridge regression are regularization techniques that penalize the magnitude of the coefficients to prevent overfitting. They are useful when dealing with high-dimensional data or multicollinearity.\n",
        "\n",
        "7.ElasticNet Regression:\n",
        "   - ElasticNet combines the penalties of Lasso and Ridge regression, offering a balance between variable selection (L1 penalty) and coefficient shrinkage (L2 penalty). It is effective in situations where both feature selection and regularization are desired.\n",
        "\n",
        "8.XGBoost and LightGBM:\n",
        "   - XGBoost and LightGBM are gradient boosting frameworks known for their efficiency and high performance. They offer enhancements over traditional gradient boosting methods and are widely used in regression tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "NruXR0sA0YIG"
      }
    }
  ]
}